{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career Exploration Final Project: TMDB Box Office Prediction\n",
    "\n",
    "### Table Of Contents\n",
    "\n",
    "* [1. Exploratory Data Analysis](#eda)\n",
    "* [2. Feature Engineering and Data Cleaning](#feature-engineering)\n",
    "* [3. Modeling](#modeling)\n",
    "    * [3.1 Validation and Evaluation](#validation)\n",
    "    * [3.2 Linear Regression](#linear-regression)\n",
    "    * [3.3 Regularized Regression](#reg)\n",
    "    * [3.4 Random Forest](#random-forest)\n",
    "    * [3.5 Neural Network](#nn)\n",
    "    * [3.6 XGBoost](#xgb)\n",
    "\n",
    "\n",
    "### Hosted by and maintained by the [Students Association of Applied Statistics (SAAS)](https://saas.berkeley.edu).  Authored by [Ajay Raj](mailto:araj@berkeley.edu).\n",
    "\n",
    "For your final project in Career Exploration, you will be participating in a **Kaggle competition**, a data science and machine learning competition where you use *real* data and develop models to solve *real* problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "The problem: given data about a movie (runtime, budget, cast, crew), predict the **overall worldwide box office revenue** it will make.\n",
    "\n",
    "You'll be competing in [this Kaggle competition](https://www.kaggle.com/c/tmdb-box-office-prediction). Note that this competition has already completed, so you won't be competing against other Kagglers, but you'll be competing against your fellow CXers on a private leaderboard. For information on where the training data came from and how you're predictions are evaluated (turned into a score), check out the Kaggle competition link.\n",
    "\n",
    "**Note:** There is not much guidance provided in this project (on purpose). You'll be doing a lot of going through [previous lectures](https://github.com/SUSA-org/Spring-2019-Career-Exploration/blob/master/CX-Final-Project/CX-Final-Project-Starter.ipynb) to try to adapt the code provided there to this dataset, and reading documentation that's been linked in most of the problems. We are pushing you, fledgling data scientists, out of the nest and letting you spread your wings and fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. Create a Kaggle account at kaggle.com\n",
    "2. Go to the [Kaggle competition page](https://www.kaggle.com/c/tmdb-box-office-prediction) and click \"Late Submission\", and register for the competition/\n",
    "3. Go to the 'Account' tab of your user profile (https://www.kaggle.com/YOUR-USERNAME/account) and select 'Create API Token'\n",
    "4. Download the `kaggle.json` file, which contains a dictionary with your Kaggle credentials\n",
    "5. Put them in the `KAGGLE_USER_DATA` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_NAME = # replace this with your team name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_USER_DATA = # looks like this {\"username\":\"ajaynraj\",\"key\":\"<REDACTED>\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.drop('revenue', axis=1), train['revenue']\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do EDA and feature engineering on a dataset, we often examine the training points and the test points together, so when you do complex feature engineering and data cleaning, you don't need to do twice or worry about your transformations not applying to test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((X_train, X_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"eda\"></span>\n",
    "\n",
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "Provide two plots that demonstrate interesting aspects of the dataset, and especially certain features' influence on the target variable, revenue.\n",
    "\n",
    "Since you won't be \"submitting\" this notebook anywhere, this part of the project is technically optional, but it is a **crucial** part of the data science process, so we *highly* recommend you do this, because it will inform how you complete the next parts of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for sick scatter plots and vivacious violin plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering and Data Cleaning\n",
    "\n",
    "Transform your data into a cleaned DataFrame with the features you believe will be the most helpful towards creating a model for the revenue from the film.\n",
    "\n",
    "In order to use the models below, you will need to make every feature **numerical**, not categorical, so you need to make sure that your output DataFrame only has numbers in it (and no NaNs!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns have data that is a little funky, so here's the libraries I imported and a few functions that I used. Feel free to use them or not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def empty_listify(lst):\n",
    "    return [] if pd.isnull(lst) else eval(lst)\n",
    "\n",
    "def pcaify(one_hot, column_prefix, num_pca_columns):\n",
    "    pca = PCA(n_components=num_pca_columns)    \n",
    "    features = pca.fit_transform(one_hot)\n",
    "    \n",
    "    return pd.DataFrame(data = features, columns = ['{0}_{1}'.format(column_prefix, i) for i in range(features.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    # change this with your own feature engineering!\n",
    "    df = df.loc[:, [\"budget\", \"popularity\", \"runtime\"]]\n",
    "    df = df.fillna(0)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up our cleaned df back into training and test\n",
    "X_train = X[:train.shape[0]]\n",
    "y_train = y_train\n",
    "X_test = X[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"modeling\"/>\n",
    "\n",
    "## 3. Modeling\n",
    "\n",
    "For each of the models we try, make sure you also run the [Prediction](#prediction) cells at the bottom, so you can submit your predictions to the competition! This is how we'll be making sure you're keeping up with the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"validation\"/>\n",
    "\n",
    "### 3.1 Validation and Evaluation\n",
    "\n",
    "Our Kaggle competition (read more [here](https://www.kaggle.com/c/tmdb-box-office-prediction/overview/evaluation) uses Root-Mean-Square-Log-Error (RMSLE). In mathematical notation, it is:\n",
    "\n",
    "$$\\text{RMSLE}(\\hat{y}, y) = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n \\log(y_i - \\hat{y}_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def evaluate(y_pred, y_true):\n",
    "    \"\"\"Returns the RMSLE(y_pred, y_true)\"\"\"\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests the previous function\n",
    "\n",
    "# If this fails, it will throw an error\n",
    "assert np.allclose(evaluate(np.array([1, 2, 3, 4]), np.array([5, 6, 7, 8])), 0.8292781201720374)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation\n",
    "\n",
    "Use the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split up your training data into a training set and a validation set. The size of the validation set should be 20% of the full training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"linear-regression\"/>\n",
    "\n",
    "### 3.2 Linear Regression\n",
    "\n",
    "Fit a linear regression model to your data and report your RMLSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating linear regression object (model)\n",
    "lm = LinearRegression()\n",
    "\n",
    "# fitting model on training sets\n",
    "lm.fit(train_X, train_y)\n",
    "\n",
    "# using model to predict on validation set\n",
    "y_valid_pred = lm.predict(valid_X)\n",
    "\n",
    "# IMPORTANT: This model is a \"dumb\" model that predicts negative values for some movie revenues\n",
    "# However, because we are using RMLSE we cannot have negative predictions\n",
    "# Ideally you create a better model that doesn't predict negative revenues\n",
    "y_valid_pred[y_valid_pred < 0] = 0\n",
    "\n",
    "# evaluating prediction on validation set\n",
    "evaluate(y_valid_pred, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"reg\" />\n",
    "\n",
    "### 3.3 Regularized Regression\n",
    "\n",
    "Fit a [LASSO regression model](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) to your data with $\\lambda = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Hyperparameter Tuning\n",
    "\n",
    "Perform [3-fold cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) on the parameter $\\lambda$, which is called **alpha** when you pass it into Lasso. Find the best parameter of $\\lambda \\in \\{0.001, 0.005, 0.01, 0.05, 0.1\\}$ and report the **RMSLE** on the validation set if you use this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "alphas = [1e-3, 5e-3, 1e-2, 5e-2, 0.1]\n",
    "\n",
    "cv_scores = np.zeros(len(alphas))\n",
    "\n",
    "for alphai, alpha in enumerate(alphas):\n",
    "    print('Training alpha =', alpha, end='\\t')\n",
    "    scores = np.zeros(5)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "        # YOUR CODE HERE\n",
    "    cv_scores[alphai] = scores.mean()\n",
    "    print('RMSLE = ', cv_scores[alphai])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alphas[np.argmax(cv_scores)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=best_alpha)\n",
    "model.fit(train_X, np.log(train_y))\n",
    "training_accuracy = # YOUR CODE HERE\n",
    "validation_accuracy = # YOUR CODE HERE\n",
    "\n",
    "print('Training accuracy', training_accuracy)\n",
    "print('Validation accuracy', validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"random-forest\"/>\n",
    "\n",
    "### 3.4 Random Forest\n",
    "\n",
    "Fit a random forest model to your data and report your RMSLE.\n",
    "\n",
    "**NOTE:** If you're finding that your model is performing worse than your linear regression, make sure you tune the parameters to the RandomForestRegressor!\n",
    "\n",
    "Try to understand what the parameters mean by looking at the Decision Trees lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"nn\" />\n",
    "\n",
    "### 3.5 Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network on the data. Report your RMSLE.\n",
    "\n",
    "**NOTE**: You will probably run into issues running this on DataHub! I would recommend downloading Anaconda and running the notebook locally. Ask us on Slack if you need help on this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"xgb\" />\n",
    "\n",
    "### 3.6 XGBoost (Stretch)\n",
    "\n",
    "Now that we've tried many different types of classifiers, it's time to bring out the big guns.\n",
    "\n",
    "Below are hyperparameters for an XGBoost model: tinker around with these to achieve the best validation score (below). Learn about what some of the hyperparameters mean [here](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train).\n",
    "\n",
    "**NOTE**: You will probably run into issues to run this on DataHub! I would recommend downloading Anaconda and running the notebook locally. Ask us on Slack if you need help on this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24a3ad9214f8c5210f69a1b6f887ce602ccd2b06"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': # YOUR CODE HERE\n",
    "    'max_depth': # YOUR CODE HERE\n",
    "    'subsample': # YOUR CODE HERE\n",
    "    'colsample_bytree': # YOUR CODE HERE\n",
    "    'silent': # YOUR CODE HERE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgb import run_xgb\n",
    "xgb_preds = run_xgb(...) # change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SUBMISSION = 'submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might have to change this to be the predictions from your model on the test set\n",
    "preds = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(data={'id': test['id'], 'revenue': preds}).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.shape[0] == test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(PATH_TO_SUBMISSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import submit_to_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = submit_to_leaderboard(\n",
    "    KAGGLE_USER_DATA, \n",
    "    TEAM_NAME, \n",
    "    path_to_submission=PATH_TO_SUBMISSION, \n",
    "    submit_to_kaggle=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
